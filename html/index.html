<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Ignite Simple</title>
    <meta name="description" content="Ignite Simple Model Autogenerated Description">
    <link rel="stylesheet" href="css/styles.css?v=1.0">
</head>

<body>
    <h1>Ignite-Simple</h1>
    <section id="initial-lr-sweep">
        <h2>Initial Learning Rate Sweep</h2>
        <p>For this section the batch size was fixed to <span id="initial-batch-size">TODO</span>.
            The model was then trained for <span id="initial-half-cycle">TODO</span> epochs with
            learning rate linearly increasing from <span id="initial-min-lr">TODO</span> to <span
                id="initial-max-lr">TODO</span> over the entire length of this training. At each
            iteration (i.e, each batch), <span id="initial-lr-num-to-val">TODO</span> points were
            selected randomly from the training set and the loss was calculated and stored (without
            propagating any gradients for these points). This was repeated for <span
                id="initial-lr-num-trials">TODO</span> different random initializations of the
            model. </p>
        <p>After this process, there was a trial dimension, x dimension (learning rates), and y
            dimension which was transformed to 1/(loss+1). The trial dimension was coalesced using
            <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp</a>. The result was then
            smoothed with a Savitzky-Golay filter with window size <span
                id="initial-lr-window-size">TODO</span> and polynomial order 1 (a linear
            convolution). The derivative of 1/(loss+1) with respect to learning rate of this
            smoothed filter was approximated as described <a
                href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.gradient.html">here</a>.
            Then the smallest set of intervals in which within each interval the derivative was
            strictly positive was calculated, such that every point where the derivative was
            positive was in one of the intervals in this set. From there, the widest interval was
            determined. If this interval had long tails, i.e., if it was possible to reduce the
            interval width without correspondingly impacting the integral of the derivative over the
            interval, this was done (see <a
                href="https://github.com/Tjstretchalot/ignite-simple/blob/master/ignite_simple/range_finder.py">range_finder.py</a>
            for details). </p>
        <p>After this process, it was concluded that learning rates between <span
                id="initial-lr-sweep-result-min">TODO</span> and <span
                id="initial-lr-sweep-result-max">TODO</span> were reasonable for the given batch
            size.</p>
        <div class="figures">
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/lr/lr_vs_perf_mean_with_fillbtwn_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    1/(loss+1) at the corresponding iteration. The gray filled region is 1.96 *
                    standard deviation, calculated element wise. </div>
            </div>
            <div class="figure">
                <img class="figure-image" src="../hparams/lr/lr_vs_perf_each_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    1/(loss+1) at the corresponding iteration. Shows each individual trial in a
                    different line.</div>
            </div>
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/lr/lr_vs_lse_smoothed_perf_then_deriv_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    derivative the 1/(loss+1) at the corresponding iteration. This is calculated
                    from the LogSumExp of the trials hence there are no error bars. </div>
            </div>
        </div>
    </section>
    <section id="batch-size-sweep">
        <h2>Batch Size Sweep</h2>
        <p>For this section, it was assumed that a good learning rate for a batch size of <span
                id="initial-batch-size">TODO</span> is <span id="initial-avg-lr">TODO</span>.
            Further, it was assumed that multiplying the batch size by any factor requires
            multiplying the learning rate by the same factor. The model was trained for <span
                id="initial-half-cycle">TODO</span> epochs with batch size linearly increasing from
            <span id="initial-min-batch">TODO</span> to <span id="initial-max-batch">TODO</span>.
            The batch size for each iteration was selected based on the number of preceeding points,
            i.e., if there were 10 points total and the range of batch sizes was (1, 2, 3, 4, 5)
            then the batch sizes would be distributed to the points as (1, 1, 2, 2, 3, 3, 4, 4, 5,
            5) and then the actual batches would be (1, 1, 2, 3) for the first epoch. The leftover
            points at the end of training (not at the end of each epoch) were clipped.</p>
        <p>After each iteration (ie. each batch), <span id="initial-batch-num-to-val">TODO</span>
            points were selected randomly from the training set and the loss was calculated and
            stored (without propagating any gradients for these points). This was repeated for <span
                id="initial-batch-num-trials">TODO</span> different random initializations of the
            model. After this process, where-ever there were batch sizes with multiple values (as in
            the above example there were multiple batches with size 1), the loss for that batch size
            was taken as the mean over each of the losses collected after a batch with that size.
        </p>
        <p>After this process, there was a trial dimension, x dimension (batch sizes), and y
            dimension which was transformed to 1/(loss+1). A good range for batch sizes was selected
            at this point using precisely the same technique as for learning rate at this stage. The
            result was that good batch sizes fall between <span
                id="batch-sweep-result-min">TODO</span> and <span
                id="batch-sweep-result-max">TODO</span>. </p>
        <section id="batch-size-sweep-fastest">
            <p>The mean of this range, <span id="batch-sweep-result">TODO</span>, was selected. </p>
        </section>
        <section id="batch-size-sweep-other">
            <p>From this range, <span id="batch-sweep-num-pts">TODO</span> points were selected from
                a distribution biased toward a higher derivative of inverse loss relative to batch
                size (i.e., a batch size with derivative 2r was twice as likely to be picked as one
                with derivative r for all real values r). The points selected were <span
                    id="batch-sweep-pts-list">TODO</span>. For each selected point, <span
                    id="batch-sweep-trials-each">TODO</span> trials were performed over different
                random initializations of the model, each for <span
                    id="initial-cycle-time">TODO</span> epochs. This corresponded precisely to one
                cycle of the learning rate. After training, the loss for the model on the entire
                training set was computed. The <a
                    href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp</a> over the trial
                dimension of the final 1/(loss+1) of each of the batch sizes was computed, and the
                best of them was selected. This corresponded to a batch size of <span
                    id="batch-sweep-result">TODO</span>. </p>
        </section>
        <div class="figures">
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/batch/batch_vs_perf_mean_with_fillbtwn_1920x1080.png"
                    alt="Inverse Loss vs Batch Size, x is batch size, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the batch size and the y-axis is the 1/(loss+1)
                    at the corresponding iteration. The gray filled region is 1.96 * standard
                    deviation, calculated element wise. </div>
            </div>
            <div class="figure">
                <img class="figure-image" src="../hparams/batch/batch_vs_perf_each_1920x1080.png"
                    alt="Inverse Loss vs Batch Size, x is batch size, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the batch size and the y-axis is the 1/(loss+1)
                    at the corresponding iteration. Shows each individual trial in a different line.
                </div>
            </div>
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/batch/batch_vs_lse_smoothed_perf_then_deriv_1920x1080.png"
                    alt="Inverse Loss vs Batch Size, x is batch size, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the batch size and the y-axis is the derivative
                    the 1/(loss+1) at the corresponding iteration. This is calculated from the
                    LogSumExp of the trials hence there are no error bars. </div>
            </div>
        </div>
    </section>
    <section id="no-second-lr-sweep">
        <h2>Final Learning Rate</h2>
        <p>The final learning rate was simply the learning rate found in the initial sweep modified
            to account for the new batch size linearly. Hence, the final learning rate range was
            from <span id="lr-sweep-min">TODO</span> to <span id="lr-sweep-max">TODO</span></p>
    </section>
    <section id="second-lr-sweep">
        <h2>Second Learning Rate Sweep</h2>
        <p>For this section the batch size was fixed to <span id="batch-sweep-result">TODO</span>,
            the value from the batch size sweep. The model was then trained for <span
                id="initial-half-cycle">TODO</span> epochs with learning rate linearly increasing
            from <span id="initial-min-lr">TODO</span> to <span id="initial-max-lr">TODO</span> over
            the entire length of this training. At each iteration (i.e, each batch), <span
                id="initial-lr-num-to-val">TODO</span> points were selected randomly from the
            training set and the loss was calculated and stored (without propagating any gradients
            for these points). This was repeated for <span id="second-lr-num-trials">TODO</span>
            different random initializations of the model. </p>
        <p>After this process, there was a trial dimension, x dimension (learning rates), and y
            dimension which was transformed to 1/(loss+1). The trial dimension was coalesced using
            <a href="https://en.wikipedia.org/wiki/LogSumExp">LogSumExp</a>. The result was then
            smoothed with a Savitzky-Golay filter with window size <span
                id="second-lr-window-size">TODO</span> and polynomial order 1 (a linear
            convolution). The derivative of 1/(loss+1) with respect to learning rate of this
            smoothed filter was approximated as described <a
                href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.gradient.html">here</a>.
            Then the smallest set of intervals in which within each interval the derivative was
            strictly positive was calculated, such that every point where the derivative was
            positive was in one of the intervals in this set. From there, the widest interval was
            determined. If this interval had long tails, i.e., if it was possible to reduce the
            interval width without correspondingly impacting the integral of the derivative over the
            interval, this was done (see <a
                href="https://github.com/Tjstretchalot/ignite-simple/blob/master/ignite_simple/range_finder.py">range_finder.py</a>
            for details). </p>
        <p>After this process, it was concluded that learning rates between <span
                id="lr-sweep-result-min">TODO</span> and <span id="lr-sweep-result-max">TODO</span>
            were reasonable for the given batch size.</p>
        <div class="figures">
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/lr2/lr_vs_perf_mean_with_fillbtwn_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    1/(loss+1) at the corresponding iteration. The gray filled region is 1.96 *
                    standard deviation, calculated element wise. </div>
            </div>
            <div class="figure">
                <img class="figure-image" src="../hparams/lr2/lr_vs_perf_each_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    1/(loss+1) at the corresponding iteration. Shows each individual trial in a
                    different line.</div>
            </div>
            <div class="figure">
                <img class="figure-image"
                    src="../hparams/lr2/lr_vs_lse_smoothed_perf_then_deriv_1920x1080.png"
                    alt="Inverse Loss vs Learning Rate, x is learning rate, y is 1/(loss+1)" />
                <div class="caption"> The x-axis is the learning rate and the y-axis is the
                    derivative the 1/(loss+1) at the corresponding iteration. This is calculated
                    from the LogSumExp of the trials hence there are no error bars. </div>
            </div>
        </div>
    </section>
    <section id="trials">
        <h2>Trials</h2>
        <p>Training was performed using minibatched SGD without momentum or dampening. Learning rate
            oscillated linearly, starting at <span id="lr-sweep-result-min">TODO</span> and
            increasing to <span id="lr-sweep-result-max">TODO</span> at the start of epoch <span
                id="initial-half-cycle">TODO</span> and then decreasing back to <span
                id="lr-sweep-result-min">TODO</span> at the start of cycle <span
                id="initial-cycle-time">TODO</span>. The batch size was fixed to <span
                id="batch-sweep-result">TODO</span>. A total of <span id="trials-count">TODO</span>
            trials were performed. On the training set, after training, the highest performance was
            <span id="trials-train-best-performance">TODO</span> and the lowest loss was <span
                id="trials-train-best-loss">TODO</span>, while on average performance was <span
                id="trials-train-mean-performance">TODO</span>±<span
                id="trials-train-std-performance">TODO</span> and loss was <span
                id="trials-train-mean-loss">TODO</span>±<span
                id="trials-train-std-loss">TODO</span>. On the validation set, after training, the
            highest performance was <span id="trials-val-best-performance">TODO</span> and the
            lowest loss was <span id="trials-val-best-loss">TODO</span>, while on average
            performance was <span id="trials-val-mean-performance">TODO</span>±<span
                id="trials-val-std-performance">TODO</span> and loss was <span
                id="trials-val-mean-loss">TODO</span>±<span id="trials-val-std-loss">TODO</span>.
        </p>
        <p>Average values are presented as mean ± standard deviation.</p>
        <div class="figures">
            <div class="figure">
                <img class="figure-image"
                    src="../trials/epoch_vs_perf_train_mean_with_fillbtwn_1920x1080.png"
                    alt="Epoch vs performance, training, (mean with fill between)" />
                <div class="caption">The x-axis is the epoch and the y-axis is the performance. The
                    blue line is the mean and the grey shaded region is 1.96 * standard deviation.
                    For all points except the final one, snapshots are performed on a random sample
                    of the training set of at most 1024 points. </div>
            </div>
            <div class="figure">
                <img class="figure-image" src="../trials/epoch_vs_perf_train_each_1920x1080.png"
                    alt="Epoch vs performance, validation, (each trial as a separate line)" />
                <div class="caption">The x-axis is the epoch and the y-axis is the performance. Each
                    line corresponds to a different trial. Snapshots are performed on a random
                    sample of the training set of at most 1024 points.</div>
            </div>
            <img class="figure-image"
                src="../trials/epoch_vs_loss_val_mean_with_fillbtwn_1920x1080.png"
                alt="Epoch vs loss, training, (mean with fill between)" />
            <div class="caption">The x-axis is the epoch and the y-axis is the loss. The blue line
                is the mean and the grey shaded region is 1.96 * standard deviation. Snapshots are
                performed on a random sample of the validation set of at most 1024 points.</div>
            <div class="figure">
                <img class="figure-image" src="../trials/epoch_vs_loss_val_each_1920x1080.png"
                    alt="Epoch vs performance (each trial as a separate line)" />
                <div class="caption">The x-axis is the epoch and the y-axis is the performance. Each
                    line corresponds to a different trial. Snapshots are performed on a random
                    sample of the validation set of at most 1024 points.</div>
            </div>
        </div>
    </section>
</body>

</html>